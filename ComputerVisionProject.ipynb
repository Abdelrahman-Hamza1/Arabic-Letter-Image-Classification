{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This Notebook is Prepared by Abdelrahman Hamza - 1200020\n",
        "The purpose of the model we're building is the classification of Handwritten Arabic Letters (28) using a CNN."
      ],
      "metadata": {
        "id": "cakkwE3Nykts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first code segment is used to upload the input data (Training & Testing Data). The data should be uploaded as a zipped folder consisting of 2 folders, train & test where each folder contains the train & test images named with the following format: Id_{Image Id}_label_{label Id}.jpg"
      ],
      "metadata": {
        "id": "bo9EMuUUy5Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload zipped folder containing 2 directories: Test & Train (Images only)\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "uploaded_zip_file_name = list(uploaded.keys())[0]\n",
        "\n",
        "zip_destination_directory = '/content/'\n",
        "\n",
        "shutil.move(uploaded_zip_file_name, os.path.join(zip_destination_directory, uploaded_zip_file_name))\n",
        "\n",
        "moved_zip_file_path = os.path.join(zip_destination_directory, uploaded_zip_file_name)\n",
        "\n",
        "extraction_path = '/content/'\n",
        "\n",
        "os.makedirs(extraction_path, exist_ok=True)\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(moved_zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extraction_path)\n",
        "\n",
        "extracted_contents = os.listdir(extraction_path)\n",
        "print(f\"Contents of the extraction directory: {extracted_contents}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "cellView": "form",
        "id": "H2Rx1HLBo2xt",
        "outputId": "acbc8ba8-fc74-42ee-b2ec-5ca0623ed66b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-eaadbb83-e4e5-458f-9e33-feb1786ab425\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-eaadbb83-e4e5-458f-9e33-feb1786ab425\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving trainAndTest.zip to trainAndTest.zip\n",
            "Contents of the extraction directory: ['.config', 'trainAndTest.zip', 'train', 'test', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next segment handles formatting the data read by the first part in a way that is readable to tensorflow.keras\n",
        "where it would split the images into subdirectory based on thier label which is provided via the file name."
      ],
      "metadata": {
        "id": "wT9N_WUezfxl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CkO_-T4xag7y",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Organize Images in subdirectory format\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "def organize_images(source_directory, destination_directory):\n",
        "    # Create the destination directory if it doesn't exist\n",
        "    os.makedirs(destination_directory, exist_ok=True)\n",
        "\n",
        "    # List all files in the source directory\n",
        "    all_files = os.listdir(source_directory)\n",
        "\n",
        "    # Filter only the image files (assuming they have a common extension like '.png')\n",
        "    image_files = [file for file in all_files if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    # Define a regular expression pattern to extract label\n",
        "    pattern = re.compile(r\"id_\\d+_label_(\\d+)\\.png\")\n",
        "\n",
        "    # Organize images into subdirectories based on labels\n",
        "    for image_file in image_files:\n",
        "        match = pattern.match(image_file)\n",
        "        if match:\n",
        "            label = match.group(1)\n",
        "            label_directory = os.path.join(destination_directory, f\"class_{label}\")\n",
        "            os.makedirs(label_directory, exist_ok=True)\n",
        "\n",
        "            # Move the image file to the corresponding label subdirectory\n",
        "            source_path = os.path.join(source_directory, image_file)\n",
        "            destination_path = os.path.join(label_directory, image_file)\n",
        "            shutil.move(source_path, destination_path)\n",
        "\n",
        "# Organize train images\n",
        "train_source_directory = '/content/train'\n",
        "train_destination_directory = '/content/organized_train'\n",
        "organize_images(train_source_directory, train_destination_directory)\n",
        "\n",
        "# Organize test images\n",
        "test_source_directory = '/content/test'\n",
        "test_destination_directory = '/content/organized_test'\n",
        "organize_images(test_source_directory, test_destination_directory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next segment is for the required Imports."
      ],
      "metadata": {
        "id": "FaB8VdxxzwDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "rnAjcIJg8Ahc",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next two Segments are used to Load Data into tensorflow/keras.\n",
        "the first segment uploads the data as it is (only normalizes the pixels)\n",
        "the send segment uploads the data and does data augmentation on it.\n",
        "\n",
        "Choosing which segment to run depends on the part that we're doing.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "we can also modify some parameters in these blocks such as Batch Size, Input Image desired size & learning rate."
      ],
      "metadata": {
        "id": "nEmV63BSzzT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Data to Tensorflow\n",
        "batchSize =50\n",
        "learningRate = 0.001\n",
        "size = (32,32)\n",
        "train = tf.keras.utils.image_dataset_from_directory('/content/organized_train', batch_size=batchSize, color_mode='grayscale', image_size=size )\n",
        "train = train.map(lambda x,y: (x/255, y))\n",
        "\n",
        "test = tf.keras.utils.image_dataset_from_directory('/content/organized_test' ,color_mode='grayscale', image_size= size)\n",
        "test = test.map(lambda x,y: (x/255, y))"
      ],
      "metadata": {
        "id": "pwIu8qi_n-WU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11d9c7e-3214-4701-bd4c-d96a8cbfcd5c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 13440 files belonging to 28 classes.\n",
            "Found 3360 files belonging to 28 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Data to Tensorflow (Data Augmentation)\n",
        "\n",
        "batchSize = 50\n",
        "learningRate = 0.0001\n",
        "size = (32, 32)\n",
        "sizePreTrained = (28,28)\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=5,\n",
        "    shear_range=15,\n",
        "    zoom_range=0.2,\n",
        "    rescale=1./255\n",
        "    )\n",
        "\n",
        "train = datagen.flow_from_directory(\n",
        "    directory='/content/organized_train',\n",
        "    class_mode=\"sparse\",\n",
        "    target_size= sizePreTrained,\n",
        "    color_mode='grayscale',\n",
        "    batch_size = batchSize\n",
        ")\n",
        "\n",
        "test = tf.keras.utils.image_dataset_from_directory(\n",
        "    '/content/organized_test',\n",
        "    image_size=sizePreTrained,\n",
        "    color_mode='grayscale'\n",
        ").map(lambda x, y: (x / 255, y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "BBLXxeBBO_QW",
        "outputId": "737797d0-9fd8-4fc8-e1bf-0635fd0869fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 13440 images belonging to 28 classes.\n",
            "Found 3360 files belonging to 28 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next three segments are used to build the CNN.\n",
        "\n",
        "---\n",
        "1. The first segment builds CNN-14 (Link). which was chosen due to High accuracy as well as Model simplicity and low number of trainable parameter.\n",
        "\n",
        "2. The 2nd segment defines a Ready Model (DenseNet121)\n",
        "\n",
        "3. The 3rd segment asks to upload a pre-trained CNN to do transfer learning (takes a .h5 file which will be attached with the submission)\n",
        "\n",
        "We will run one of the next 3 segments depending on which task we're doing.\n"
      ],
      "metadata": {
        "id": "_jb4JgtX0agD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Build CNN\n",
        "num_classes = 28\n",
        "\n",
        "input_size = (32,32,1)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_size, padding='same'))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(64, (3, 3), activation='relu' , padding='same'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu' , padding='same'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu' , padding='same'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu' , padding='same'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=learningRate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Y18OdOtsCP3y",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd35f931-4c9a-496f-e7b7-6205eb2bbbda"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        320       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 16, 16, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 64)          36928     \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 64)          36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4, 4, 64)          0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 2, 2, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 2, 2, 64)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 28)                7196      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 351484 (1.34 MB)\n",
            "Trainable params: 351484 (1.34 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Build CNN (Ready Model)\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "\n",
        "model = DenseNet121(\n",
        "    include_top=True,\n",
        "    weights=None,\n",
        "    input_tensor=None,\n",
        "    input_shape=(32,32,1),\n",
        "    pooling=None,\n",
        "    classes=28,\n",
        "    classifier_activation=\"softmax\",\n",
        ")\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=learningRate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SqsBoGytPEQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Pre-Trained Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, InputLayer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "num_classes = 28\n",
        "\n",
        "modelFile = files.upload()\n",
        "filename = list(modelFile.keys())[0]\n",
        "base_model = load_model(filename)\n",
        "\n",
        "base_model.layers[0] = InputLayer(input_shape=(28, 28, 1))\n",
        "\n",
        "base_model.layers[-1] = Dense(28, activation='softmax')\n",
        "\n",
        "# i = 0\n",
        "# for layer in base_model.layers:\n",
        "#   layer.trainable = False\n",
        "#   i+=1\n",
        "#   if i == 4: break\n",
        "\n",
        "model = base_model\n",
        "\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile the new model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "knwBz1vgFVww",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next segment trains the model. It also prints the logs to show the progression of each model as it trains.\n",
        "\n",
        "We can define the number of Epochs as a parameter to the fit function in this code segment."
      ],
      "metadata": {
        "id": "bbBMPopl1aii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train Model\n",
        "logdir='logs'\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "hist = model.fit(train, epochs=100, batch_size=batchSize, validation_data=test, callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "id": "bvnqQH85DE_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db7ee9bf-49d4-4165-c53c-1353ba2d6321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "269/269 [==============================] - 12s 15ms/step - loss: 3.1273 - accuracy: 0.0676 - val_loss: 2.4507 - val_accuracy: 0.1542\n",
            "Epoch 2/100\n",
            "269/269 [==============================] - 4s 14ms/step - loss: 1.9577 - accuracy: 0.2847 - val_loss: 1.2926 - val_accuracy: 0.4664\n",
            "Epoch 3/100\n",
            "269/269 [==============================] - 3s 12ms/step - loss: 1.3354 - accuracy: 0.4545 - val_loss: 1.0530 - val_accuracy: 0.5622\n",
            "Epoch 4/100\n",
            "269/269 [==============================] - 4s 15ms/step - loss: 0.9998 - accuracy: 0.5968 - val_loss: 0.6510 - val_accuracy: 0.7634\n",
            "Epoch 5/100\n",
            "269/269 [==============================] - 4s 14ms/step - loss: 0.6803 - accuracy: 0.7504 - val_loss: 0.4794 - val_accuracy: 0.8417\n",
            "Epoch 6/100\n",
            "269/269 [==============================] - 3s 11ms/step - loss: 0.5326 - accuracy: 0.8193 - val_loss: 0.3628 - val_accuracy: 0.8753\n",
            "Epoch 7/100\n",
            "269/269 [==============================] - 3s 11ms/step - loss: 0.3977 - accuracy: 0.8740 - val_loss: 0.2678 - val_accuracy: 0.9199\n",
            "Epoch 8/100\n",
            "269/269 [==============================] - 4s 14ms/step - loss: 0.3399 - accuracy: 0.8963 - val_loss: 0.3311 - val_accuracy: 0.9074\n",
            "Epoch 9/100\n",
            "269/269 [==============================] - 3s 12ms/step - loss: 0.3018 - accuracy: 0.9114 - val_loss: 0.2469 - val_accuracy: 0.9301\n",
            "Epoch 10/100\n",
            "269/269 [==============================] - 3s 12ms/step - loss: 0.2582 - accuracy: 0.9243 - val_loss: 0.3107 - val_accuracy: 0.9098\n",
            "Epoch 11/100\n",
            "269/269 [==============================] - 3s 12ms/step - loss: 0.2314 - accuracy: 0.9350 - val_loss: 0.2649 - val_accuracy: 0.9295\n",
            "Epoch 12/100\n",
            "269/269 [==============================] - 4s 13ms/step - loss: 0.2151 - accuracy: 0.9399 - val_loss: 0.2364 - val_accuracy: 0.9390\n",
            "Epoch 13/100\n",
            "269/269 [==============================] - 4s 13ms/step - loss: 0.2056 - accuracy: 0.9429 - val_loss: 0.2633 - val_accuracy: 0.9259\n",
            "Epoch 14/100\n",
            "269/269 [==============================] - 3s 11ms/step - loss: 0.1784 - accuracy: 0.9501 - val_loss: 0.2989 - val_accuracy: 0.9232\n",
            "Epoch 15/100\n",
            "269/269 [==============================] - 3s 12ms/step - loss: 0.1587 - accuracy: 0.9541 - val_loss: 0.2935 - val_accuracy: 0.9277\n",
            "Epoch 16/100\n",
            "269/269 [==============================] - 4s 14ms/step - loss: 0.1680 - accuracy: 0.9545 - val_loss: 0.2538 - val_accuracy: 0.9357\n",
            "Epoch 17/100\n",
            "269/269 [==============================] - 3s 11ms/step - loss: 0.1492 - accuracy: 0.9600 - val_loss: 0.2525 - val_accuracy: 0.9411\n",
            "Epoch 18/100\n",
            "269/269 [==============================] - 3s 11ms/step - loss: 0.1434 - accuracy: 0.9620 - val_loss: 0.2628 - val_accuracy: 0.9449\n",
            "Epoch 19/100\n",
            "269/269 [==============================] - 5s 17ms/step - loss: 0.1377 - accuracy: 0.9654 - val_loss: 0.2878 - val_accuracy: 0.9336\n",
            "Epoch 20/100\n",
            "269/269 [==============================] - 3s 11ms/step - loss: 0.1329 - accuracy: 0.9652 - val_loss: 0.2879 - val_accuracy: 0.9310\n",
            "Epoch 21/100\n",
            "269/269 [==============================] - 3s 12ms/step - loss: 0.1227 - accuracy: 0.9661 - val_loss: 0.2265 - val_accuracy: 0.9485\n",
            "Epoch 22/100\n",
            "269/269 [==============================] - 4s 14ms/step - loss: 0.1282 - accuracy: 0.9658 - val_loss: 0.2119 - val_accuracy: 0.9497\n",
            "Epoch 23/100\n",
            "269/269 [==============================] - 3s 12ms/step - loss: 0.1335 - accuracy: 0.9650 - val_loss: 0.2023 - val_accuracy: 0.9557\n",
            "Epoch 24/100\n",
            "269/269 [==============================] - 4s 14ms/step - loss: 0.1189 - accuracy: 0.9700 - val_loss: 0.2418 - val_accuracy: 0.9488\n",
            "Epoch 25/100\n",
            "269/269 [==============================] - 3s 12ms/step - loss: 0.0978 - accuracy: 0.9734 - val_loss: 0.2487 - val_accuracy: 0.9470\n",
            "Epoch 26/100\n",
            "269/269 [==============================] - 3s 11ms/step - loss: 0.1154 - accuracy: 0.9677 - val_loss: 0.2648 - val_accuracy: 0.9473\n",
            "Epoch 27/100\n",
            "269/269 [==============================] - 4s 15ms/step - loss: 0.0927 - accuracy: 0.9746 - val_loss: 0.3070 - val_accuracy: 0.9443\n",
            "Epoch 28/100\n",
            "269/269 [==============================] - 3s 12ms/step - loss: 0.1152 - accuracy: 0.9705 - val_loss: 0.3184 - val_accuracy: 0.9369\n",
            "Epoch 29/100\n",
            "269/269 [==============================] - 3s 12ms/step - loss: 0.0928 - accuracy: 0.9746 - val_loss: 0.2603 - val_accuracy: 0.9458\n",
            "Epoch 30/100\n",
            "269/269 [==============================] - 4s 14ms/step - loss: 0.0980 - accuracy: 0.9748 - val_loss: 0.2485 - val_accuracy: 0.9512\n",
            "Epoch 31/100\n",
            "269/269 [==============================] - 3s 12ms/step - loss: 0.1094 - accuracy: 0.9694 - val_loss: 0.2391 - val_accuracy: 0.9503\n",
            "Epoch 32/100\n",
            "182/269 [===================>..........] - ETA: 0s - loss: 0.0855 - accuracy: 0.9775"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next segment is used to Plot the Accuracy & Loss for both training & testing data."
      ],
      "metadata": {
        "id": "gyuK0uGu1sIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot Performace\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(hist.history['loss'], color='teal', label='training loss')\n",
        "plt.plot(hist.history['val_loss'], color='orange', label='validation loss')\n",
        "fig.suptitle('Loss', fontsize=20)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(hist.history['accuracy'], color='teal', label='train accuracy')\n",
        "plt.plot(hist.history['val_accuracy'], color='orange', label='validation accuracy')\n",
        "fig.suptitle('Accuracy', fontsize=20)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vBGQa1oTD0hp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}